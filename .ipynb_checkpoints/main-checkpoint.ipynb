{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10327956-8f8c-4b5e-986d-58cba61d2ae5",
   "metadata": {},
   "source": [
    "# Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84061e73-0f25-4851-bff2-aa51f8f117b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "f= open('train_20230909.jsonl','r',encoding='utf-8')\n",
    "lines=f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ee03c2c-be50-4304-88bc-f73b1ee24a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=[]\n",
    "for line in lines:\n",
    "    json_data=json.loads(line)\n",
    "    dataset.append(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4a0bd82-9aeb-4c36-9637-d61c08cada12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>cái</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>đèn</td>\n",
       "      <td>O</td>\n",
       "      <td>B-dv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>tranh</td>\n",
       "      <td>O</td>\n",
       "      <td>E-dv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>trong</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>nhà</td>\n",
       "      <td>O</td>\n",
       "      <td>B-loc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sentence #   Word POS    Tag\n",
       "0          0    cái   O      O\n",
       "1          0    đèn   O   B-dv\n",
       "2          0  tranh   O   E-dv\n",
       "3          0  trong   O      O\n",
       "4          0    nhà   O  B-loc"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from process_data.processor import *\n",
    "\n",
    "p=processor()\n",
    "NER_dataframe=p.create_csv(dataset)\n",
    "NER_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e92da61a-b4de-4689-babf-6905b286c0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_dataframe.to_csv('NER_Data_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54610561-8885-4b8f-9d8c-541e627c1979",
   "metadata": {},
   "source": [
    "# NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba66152c-6d83-41b1-b79f-2d5ef3f4059c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f7411d4-098c-44f7-9ea9-d1f8e4c863f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('NER_Data_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70a10cce-69b5-450c-a07e-3ab88df79652",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>cái</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>đèn</td>\n",
       "      <td>O</td>\n",
       "      <td>B-dv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>tranh</td>\n",
       "      <td>O</td>\n",
       "      <td>E-dv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>trong</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>nhà</td>\n",
       "      <td>O</td>\n",
       "      <td>B-loc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82726</th>\n",
       "      <td>7489</td>\n",
       "      <td>2</td>\n",
       "      <td>O</td>\n",
       "      <td>B-dr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82727</th>\n",
       "      <td>7489</td>\n",
       "      <td>tiếng</td>\n",
       "      <td>O</td>\n",
       "      <td>I-dr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82728</th>\n",
       "      <td>7489</td>\n",
       "      <td>56</td>\n",
       "      <td>O</td>\n",
       "      <td>I-dr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82729</th>\n",
       "      <td>7489</td>\n",
       "      <td>phút</td>\n",
       "      <td>O</td>\n",
       "      <td>E-dr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82730</th>\n",
       "      <td>7489</td>\n",
       "      <td>đi</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82731 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sentence #   Word POS    Tag\n",
       "0               0    cái   O      O\n",
       "1               0    đèn   O   B-dv\n",
       "2               0  tranh   O   E-dv\n",
       "3               0  trong   O      O\n",
       "4               0    nhà   O  B-loc\n",
       "...           ...    ...  ..    ...\n",
       "82726        7489      2   O   B-dr\n",
       "82727        7489  tiếng   O   I-dr\n",
       "82728        7489     56   O   I-dr\n",
       "82729        7489   phút   O   E-dr\n",
       "82730        7489     đi   O      O\n",
       "\n",
       "[82731 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5413599a-0e18-4839-b111-d1b4daf28d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    def __init__(self, data):\n",
    "        self.n_sent=1\n",
    "        self.data=data\n",
    "        self.empty=False\n",
    "        agg_func=lambda s: [(w, t) for w, t in zip(s[\"Word\"].values.tolist(),s[\"Tag\"].values.tolist())]\n",
    "        self.grouped=self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences=[s for s in self.grouped]\n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s=self.grouped[\"{}\".format(self.n_sent)]\n",
    "            self.n_sent +=1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "565e5b53-1cc3-4f24-baa8-94fca6781518",
   "metadata": {},
   "outputs": [],
   "source": [
    "getter = SentenceGetter(data)\n",
    "sentences=[[word[0] for word in sentence] for sentence in getter.sentences]\n",
    "labels=[[tag[1] for tag in sentence] for sentence in getter.sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1efe1195-eddc-450a-a4a9-ecd31ce04fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_values=list(set(data['Tag'].values))\n",
    "tag_values.append('PAD')\n",
    "tag2idx= {t: i for i, t in enumerate(tag_values)}\n",
    "idx2tag = {v: k for k, v in tag2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd659ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I-dr',\n",
       " 'I-ta',\n",
       " 'I-loc',\n",
       " 'E-dv',\n",
       " 'E-cm',\n",
       " 'B-dr',\n",
       " 'B-dv',\n",
       " 'I-sc',\n",
       " 'I-dv',\n",
       " 'E-ta',\n",
       " 'E-dr',\n",
       " 'B-cv',\n",
       " 'B-loc',\n",
       " 'B-sc',\n",
       " 'I-tn',\n",
       " 'I-cv',\n",
       " 'E-cv',\n",
       " 'O',\n",
       " 'B-ta',\n",
       " 'E-loc',\n",
       " 'E-sc',\n",
       " 'B-cm',\n",
       " 'E-tn',\n",
       " 'B-tn',\n",
       " 'PAD']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7fad6e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('idx2tag.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(idx2tag, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9365a6a3-5028-4549-985d-038799de85c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d1b73cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 25\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "066dd44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer =BertTokenizer.from_pretrained('viet_bert_tokenizer', do_lower_case = False)\n",
    "#tokenizer.save_pretrained(\"./viet_bert_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77920e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_preserver_labels(sentence, text_labels):\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "    for word, label in zip(sentence, text_labels):\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "        \n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "        labels.extend([label] * n_subwords)\n",
    "    return tokenized_sentence, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b9e8c298",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_and_preserver_labels = [\n",
    "    tokenize_and_preserver_labels(sent, labs)\n",
    "    for sent, labs in zip(sentences, labels)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "31646081",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = [token_labels_pair[0] for token_labels_pair in tokenized_and_preserver_labels]\n",
    "labels = [token_labels_pair[1] for token_labels_pair in tokenized_and_preserver_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6caff1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
    "                          truncating='post', padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd686318",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
    "                     maxlen = MAX_LEN, value = tag2idx[\"PAD\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating = \"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f93757f2-20be-45a4-8b01-2c717ca6b156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from data_processor_pipeline.custom_dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "60531e7e-848b-40cf-8ffd-37b61cdbc701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csdt=Custom_Dataset(sentences, labels, is_save_vocab=True, max_length=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "76da6772-dd65-4eec-8d53-c00715833a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = pad_sequences(csdt.numericalized,\n",
    "#                           maxlen=25, dtype=\"long\", value=0.0,\n",
    "#                           truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7420c416-d743-431d-b577-722ec5d02971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
    "#                      maxlen=25, value=tag2idx[\"PAD\"], padding=\"post\",\n",
    "#                      dtype=\"long\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "174f758e-b160-45d4-a0c4-db35e2d26f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags,\n",
    "                                                            random_state=2018, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9f4612de-bc3b-4a0a-846e-45fff3f8d2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_inputs = torch.tensor(tr_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "tr_tags = torch.tensor(tr_tags)\n",
    "val_tags = torch.tensor(val_tags)\n",
    "#tr_masks = torch.tensor(tr_masks)\n",
    "#val_masks = torch.tensor(val_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "33edf5ab-0938-4a87-962e-1d71137b011b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(tr_inputs, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=16)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c89e26-de1e-4b8d-b2b2-4781cd64cf68",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a1b5289c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import AdamW\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from build_model.build_rnn import *\n",
    "from trainer.trainer import *\n",
    "import shutil\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ad36459-843e-454a-afa9-6c7c8b286191",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_RNN=RNN(30000, embedding_dim=768, output_dim=len(tag2idx)).to(device)\n",
    "# with open('model_architecture/RNN.pkl', 'wb') as f:\n",
    "#     pickle.dump(model_RNN, f)\n",
    "optimizer_RNN = AdamW(\n",
    "    model_RNN.parameters(),\n",
    "    lr=0.001,\n",
    "    eps=1e-8\n",
    ")\n",
    "epochs = 3\n",
    "max_grad_norm = 1.0\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer_RNN,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "_trainer = trainer()\n",
    "if os.path.exists('logs/RNN'):\n",
    "    shutil.rmtree('logs/RNN')\n",
    "writer = SummaryWriter(log_dir='logs/RNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ad1ed7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN | Epoch: 1/3 | Iter: 413/413 | Error: 0/413 | Loss: 0.0220: 100%|\u001b[38;2;128;0;128m███████████████\u001b[0m| 413/413 [01:42<00:00,  4.02it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST | Epoch: 1/3 | Loss: 0.1384 | Accuracy: 0.9532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN | Epoch: 2/3 | Iter: 413/413 | Error: 0/413 | Loss: 0.0088: 100%|\u001b[38;2;128;0;128m███████████████\u001b[0m| 413/413 [01:41<00:00,  4.06it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST | Epoch: 2/3 | Loss: 0.1179 | Accuracy: 0.9563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN | Epoch: 3/3 | Iter: 413/413 | Error: 0/413 | Loss: 0.0081: 100%|\u001b[38;2;128;0;128m███████████████\u001b[0m| 413/413 [01:44<00:00,  3.95it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST | Epoch: 3/3 | Loss: 0.1135 | Accuracy: 0.9575\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "for epoch in range(epochs):\n",
    "    _trainer.train(model_RNN, train_dataloader, epoch, epochs, writer, criterion, optimizer_RNN, scheduler, device, len(tag2idx), max_grad_norm)\n",
    "    val_loss, val_acc = _trainer.validation(model_RNN, valid_dataloader, criterion, device, len(tag2idx))\n",
    "    print(f\"TEST | Epoch: {epoch+1}/{epochs} | Loss: {val_loss:.4f} | Accuracy: {val_acc:.4f}\")\n",
    "    writer.add_scalar('Val/Loss', val_loss, epoch+1)\n",
    "    writer.add_scalar('Val/Acc', val_acc, epoch+1)\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model_RNN.state_dict(),\n",
    "        'epoch': epoch+1,\n",
    "        'opimizer_state_dict': optimizer_RNN.state_dict(),\n",
    "    }\n",
    "    os.makedirs('model_state_dict/RNN', exist_ok=True)\n",
    "    torch.save(checkpoint, os.path.join('model_state_dict/RNN','last.pth'))\n",
    "    if val_acc>best_acc:\n",
    "        torch.save(checkpoint, os.path.join('model_state_dict/RNN','best.pth'))\n",
    "        best_acc=val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88fb51ab-8521-4648-ab36-24aeaac164ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_TAGS = {\n",
    "  'cv':'changing value',\n",
    "  'cm':'command',\n",
    "  'dv':'device',\n",
    "  'dr':'duration',\n",
    "  'loc':'location',\n",
    "  'sc':'scene',\n",
    "  'tn':'target number',\n",
    "  'ta':'time at'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3377950e-5f23-4069-aaaa-b648f3ffed6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(tokens, labels):\n",
    "    entities = []\n",
    "    current_entity = None\n",
    "\n",
    "    for token, label in zip(tokens, labels):\n",
    "        label_type = label.split(\"-\")[1] if \"-\" in label else None\n",
    "        if label_type:\n",
    "            label_type = RE_TAGS[label_type]\n",
    "            if current_entity and label_type != current_entity[\"type\"]:\n",
    "                entities.append(current_entity)\n",
    "                current_entity = None\n",
    "\n",
    "            if not current_entity:\n",
    "                current_entity = {\"type\": label_type, \"filler\": token}\n",
    "            else:\n",
    "                current_entity[\"filler\"] += \" \" + token\n",
    "        elif current_entity:\n",
    "            entities.append(current_entity)\n",
    "            current_entity = None\n",
    "    if current_entity:\n",
    "        entities.append(current_entity)\n",
    "\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04632d02-2c58-498a-980e-829ced5520eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processor_pipeline.vectorizer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "426ae3b3-5262-4bc2-a31e-52b6e301753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_char(text_sentence):\n",
    "  words = re.findall(r'\\w+|\\S', text_sentence)\n",
    "  return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52310841-2477-41e1-828f-7e72b412a2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ner(model_ner,text_sentence, tokenizer):\n",
    "  tokenized_sentence = tokenizer.encode(text_sentence)\n",
    "  input_ids = torch.tensor([tokenized_sentence])\n",
    "  print(input_ids)\n",
    "  with torch.no_grad():\n",
    "      output = model_ner(input_ids)\n",
    "  label_indices = np.argmax(output[0].to('cpu').numpy(), axis=1)\n",
    "  tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
    "  new_tokens, new_labels = [], []\n",
    "  for token, label_idx in zip(tokens, label_indices):\n",
    "      if token.startswith(\"##\"):\n",
    "          new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "      else:\n",
    "          new_labels.append(tag_values[label_idx])\n",
    "          new_tokens.append(token)\n",
    "  new_labels.pop(0)\n",
    "  new_labels.pop(-1)\n",
    "  new_tokens = split_char(text_sentence)\n",
    "  print(text_sentence)\n",
    "  result = extract_entities(new_tokens, new_labels)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0de57093-940e-420e-ba1a-19827174308e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   2,  269,  173,  384,  248,  502, 1432, 3224,  154,   30,   45, 1144,\n",
      "          250,  307,   25, 2827,    1, 1170, 1947,    3]])\n",
      "em tăng giúp chị cái đèn chùm ở đầu hè độ sáng đến 89% thôi nhé\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'type': 'command', 'filler': 'tăng'},\n",
       " {'type': 'device', 'filler': 'đèn chùm'},\n",
       " {'type': 'location', 'filler': 'đầu hè'},\n",
       " {'type': 'changing value', 'filler': '89'}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import BertTokenizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# with open('model_architecture/RNN.pkl', 'rb') as f:\n",
    "#     model_RNN = pickle.load(f)\n",
    "model_RNN=RNN(30000, embedding_dim=768, output_dim=25).to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained('viet_bert_tokenizer',do_lower_case = False)\n",
    "model_RNN_load = torch.load('model_state_dict/RNN/model_RNN.pth', map_location=device)\n",
    "predict_ner(model_RNN_load, 'em tăng giúp chị cái đèn chùm ở đầu hè độ sáng đến 89% thôi nhé', tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c80a13ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "torch.save(model_RNN,'model_state_dict\\RNN\\model_RNN.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "94a60e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Accuracy': 0.9584761904761905,\n",
       " 'Precision': 0.7844274809160305,\n",
       " 'Recall': 0.8617913451861792,\n",
       " 'F1-Score': 0.8212915601023019,\n",
       " 'Exact Match (EM)': 0.43537414965986393}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model_RNN=RNN(30000, embedding_dim=768, output_dim=len(tag2idx)).to(device)\n",
    "#model_state_dict=torch.load('model_state_dict/RNN/model_RNN.pth', map_location=device)\n",
    "model_RNN = torch.load('model_state_dict/RNN/model_RNN.pth', map_location=device)\n",
    "_trainer.evaluate_model(model_RNN.to(device), valid_dataloader, idx2tag, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5da981-6c3a-4f39-89bb-08bba7b96b40",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cae70f52-1b34-46a8-b00f-96576b6ed507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88f0abfc-e398-4e21-b2dd-24c775277151",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 25\n",
    "bs = 32\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b00be00d-b60b-46e8-91c4-33d6e8bf9a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fe4852a846c451facfe0c23eff6684c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/238 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "175c7b79b095492aa814f9a2b3bddc78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/207k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a05bf064f14e078aebf3a0d93f6e9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f13c20c57384f2aadd28d9ba0899595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/846 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('trituenhantaoio/bert-base-vietnamese-uncased', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bd177a4e-30e4-46f4-bd79-915650dd910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_preserve_labels(sentence, text_labels):\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    for word, label in zip(sentence, text_labels):\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc9a3b99-0d32-409c-ac16-8188df31dd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts_and_labels = [\n",
    "    tokenize_and_preserve_labels(sent, labs)\n",
    "    for sent, labs in zip(sentences, labels)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a693c0f6-d3e9-4eb2-ac6c-346e81d8e8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = [token_labels_pair[0] for token_labels_pair in tokenized_texts_and_labels]\n",
    "labels = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7b3b20a7-f100-47a0-a28d-96d42a83dba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cái',\n",
       " 'đèn',\n",
       " 'tranh',\n",
       " 'trong',\n",
       " 'nhà',\n",
       " 'giữ',\n",
       " 'đồ',\n",
       " 'trường',\n",
       " 'sa',\n",
       " 'có',\n",
       " 'còn',\n",
       " 'không',\n",
       " 'ấy',\n",
       " 'nhờ',\n",
       " 'đi',\n",
       " 'kiểm',\n",
       " 'tra',\n",
       " 'ngay',\n",
       " 'nhé']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "410a78b1-869f-4af1-8ec5-604bda57bb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cái', 'đèn', 'tranh', 'trong', 'nhà', 'giữ', 'đồ', 'trường', 'sa', 'có', 'còn', 'không', 'ấy', 'nhờ', 'đi', 'kiểm', 'tra', 'ngay', 'nhé']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0cec17-7d31-4cd8-8c3a-a5d8c90ffb55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
